{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report\n",
    "## By Jiacheng Han"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preface\n",
    "\n",
    "Data Wrangling is transforming messy and dirty data into formats useful for data analysis. In this project, I data wrangled data concerning the Twitter page WeRateDogs (@dog_rates). The data includes dog photos and corresponding text and ratings that give the photos an interesting flair. In addition to the public information, I also used image prediction and JSON data to further add insight into the dog photos\n",
    "\n",
    "\n",
    "#### The Wrangle and Analyze Data Project focused on wrangling data from 3 different sources:\n",
    "\n",
    "1. Twitter archive \n",
    "2. Image prediction file \n",
    "3. JSON data saved as tweet_json.txt\n",
    "\n",
    "#### I went through the gather, assess, and cleaning phase for each DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather\n",
    "\n",
    "1. Twitter archive was downloaded manually\n",
    "2. Image prediction file was downloaded programmatically from Udacity's servers\n",
    "3. JSON data was acquired from querying Twitter API across the Twitter archive DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess\n",
    "\n",
    "The assessment portion was split between checking datasets visually and programmatically. Based on project specifications, I looked for null values and conversion errors. I split the issues into cleaning and tidiness issues that would be dealt with later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean\n",
    "\n",
    "The cleaning portion consists of defining the problem, coding the solution, and testing the results. I first made copies of the three DataFrames. Then I proceeded to remove unneeded columns and rows that were missing vital information, like image url or rating. The rating, consisting of a numerator and denominator, needed to be converted to float in order to accommodate decimals. 4 rows in the Twitter archive were removed because they didn't have a rating and were given one based on other text. The denominator was standardized to 10, and the numerators were scaled by a factor of denominator divided by 10. In the image prediction DataFrame, I only kept the first prediction and confidence level for each row. I also deleted any prediction that wasn't for a dog. \n",
    "\n",
    "#### Tidiness\n",
    "\n",
    "I also managed tidiness by merging together the three DataFrames through 2 left joins. I needed to convert the tweet ids to int64 type in order to join. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
